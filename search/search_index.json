{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Real-time inventory demonstration \u00b6 This repository uses OpenShift GitOps to manage the deployment of the real-time inventory demonstration. What you will learn \u00b6 Use Quarkus, with reactive programming API like Mutiny, and Kafka API to produce messages to Kafka Same Quarkus app can generate messages to RabbitMQ using the AMQP API Same Quarkus app can generate messages to IBM MQ using JMS Use Quarkus and Kafka Streams to compute aggregates to build an inventory view from the stream of sale order events Use IBM Event Streams, Kafka Connector Use the RabbitMQ source connector from IBM Event messaging open source contribution Use the IBM MQ source connector from IBM Event messaging open source contribution Why to consider \u00b6 This project can be a good foundation to discuss GitOps deployment, and reuse scripts, makefile... to deploy any event-driven solution. As a developer you will use Microprofile reactive messaging, Kafka Streams API, Quarkus and OpenLiberty. Kafka IBM MQ source connector from IBM Event messaging github, Elastic search sink connector... Use Case Overview \u00b6 Today, a lot of companies which are managing item / product inventory are facing real challenges to get a close to real-time view of item availability and global inventory view. The solution can be very complex to implement while integrating Enterprise Resource Planning products and other custom legacy systems. Any new solutions are adopting events as a source to exchange data, to put less pressure on existing ERP servers, and to get better visibility into inventory positions while bringing agility to develop new solution with streaming components. This scenario implements a simple near real-time inventory management solution based on real life MVPs we developed in 2020 for different customers. The demonstration illustrates how event streaming processing help to build inventory views. A production deployment will implement different level of store and warehouse inventory aggregators that will push results as events to an event backbone, which in our case is IBM Event Streams. Servers in the Store are sending sale transactions to a central messaging platform, where streaming components are computing the different aggregates. This is a classical data streaming pipeline. Sink connectors, based on Kafka Connect framework, may be used to move data to long persistence storage like object storage or a Database, to integrate results back to Legacy ERP, to use indexing product like Elastic Search, or to propagate events to dashboards. In real life, an as-is solution will include back-end applications to manage the warehouses inventory, connected to a home-built fulfillment application, combined with store applications and servers, e-commerce suite, and a set of SOA services exposing backend systems. This is the larger view of the following figure: We may have integration flows to do data mapping, but most of those calls are synchronous. To get item availability, a lot of SOAP calls are done, increasing latency, and the risk of failure. There is an interesting video from Scott Havens explaining the needs from transitioning from a synchronous architecture to an event-driven asynchronous architecture when scaling, and low latency are must have. This lab reflects this approach. Demonstration components \u00b6 The demonstration may be used with different level of integration, and you can select those a la carte . The following diagram illustrates the current components: Diagram source: rt-inventory diagram The store simulator application is a Quarkus based microservice, used to generate item sales to different possible messaging middlewares ( RabbitMQ, IBM MQ or directly to IBM Event Streams). If you want to browse the code, the main readme of this project includes how to package and run the app with docker compose. A code explanation section may give you some ideas to developers. The docker image is quay.io/ibmcase/eda-store-simulator/ and can be used for demonstration. The item inventory aggregator is a stateful application, done with Kafka Stream API. The source code is in the refarch-eda-item-inventory project . Consider this more as a black box in the context of the scenario, it consumes items events, aggregate them, expose APIs on top of Kafka Streams interactive queries and publishes inventory events on item.inventory topic. As a developer you may want to understand the Kafka Stream programming with the following labs , and then considering looking at the classes: ItemProcessingAgent.java . The store inventory aggregator is a Kafka Stream application, also done with Kafka Stream API. The source code is in the refarch-eda-store-inventory project . The output is in store.inventory topic. When messages are sourced to Queues, then a Kafka Source Connector is used to propagate message to items topics. The MQ to Kafka Kafka connect cluster is defined in the eda-rt-inventory-GitOps repository under the kconnect folder, and the source connector in environments/rt-inventory-dev/apps/mq-source The Kafka to Cloud Object Storage Kafka (S3 bucket) connector is defined in the environments/rt-inventory-dev/apps/cos-sink folder . The Sink connector to Elastic Search is defined in environments/rt-inventory-dev/apps/elastic-sink folder . Kafka Connect is used to integrate external systems into Kafka. For example external systems can inject item sale messages to queue, from which a first MQ source Kafka connector publishes the messages to the items Kafka topic. Items sold events are processed by a series of streaming microservices which publishes aggregation results to different topics. Those topics content could be which will be used by Sink connectors to send records to other external systems. General pre-requisites \u00b6 Get access to an OpenShift Cluster. All the CLI commands must be performed by a Cluster administrator. You need oc cli and the jq JSON stream editor installed. OpenShift CLI on your local environment. jq on your local environment. Docker and docker compose to run the solution locally. git CLI . Clone this repository git clone https://github.com/ibm-cloud-architecture/eda-rt-inventory-gitops.git For OpenShift deployment, you need access to a cluster with storage capabilities to support Event Streams deployments like block storage configured to use the XFS or ext4 file system, as described in Event Streams storage . You need to have at least one volume per broker and one per zookeeper instance. See also the interactive Installation Guide for cloud pak for integration . A GitOps approach for solution deployment \u00b6 As any cloud-native and kubernetes based solution, we use continuous integration and continuous deployment practices. From a demonstration point of view, the most interesting part is to execute continuous deployment using a GitOps approach as presented in the EDA reference. See the specific explanation in this section. Choose a runtime option \u00b6 We try to make the business scenario, easily demonstrable by enablind developer's laptop execution with docker compose or use a simple free OpenShift Cluster on IBM cloud. Run on your laptop Use GitOps on a new OpenShift Cluster Use Gitops on existing Cloud Pak Integration (multi-tenant) Without GitOps, just yaml, on a new OpenShift Cluster Without GitOps, just yaml, on existing Cloud Pak Integration (multi-tenant) A non gitops approach \u00b6 Deploy on a brand new OpenShift cluster \u00b6 The makefile in this repository supports the minimum commands to use to deploy the different components: # [optional]: prepare entitlementkey, IBM catalog make prepare # [optional]: install the different cp4i operators make install_cp4i_operators # Deploy the dev environment make deploy_rt_inventory Deploy on multi-tenant environment \u00b6 The same makefile supports also to deploy to an existing Cloud Pak for Integration deployment with Event Streams being part of a namespace named cp4i-eventstreams : # Deploy all apps in rt-inventory-dev but use cp4i-eventstreams make multi_tenants Two different streaming approaches \u00b6 We propose two approaches to develop the streaming processing. One using Kafka Streams with two applications One using Apache Flink Kafka Streams implementation \u00b6 The Item-aggregator, based on Kafka Stream APIs, is in this project: refarch-eda-item-inventory The Store-aggregator, also based on Kafka Stream APIs, is in this project: refarch-eda-store-inventory Fink implementation \u00b6 See the refarch-eda-item-inventory-sql-flink repository for more information. Run the solution locally \u00b6 As a developer or technical seller, you could demonstrate this scenario on your laptop using MQ and Event Streams docker images. The docker images for each custom microservices used in this solution are in public registry ( Quay.io ). Under this repository the local-demo/kstream folder has different docker compose files to run different components: docker-compose.yaml for Event Streams, IBM MQ, Kafka Connector the Store Simulator App, the Item aggregator App, the Store aggregator App, and KafDrop to get a user interface to Kafka. docker-compose-all.yaml same as above plus ElasticSearch (1 node) and Kibana Once you have cloned the gitops repository (see pre-requisites section), go under the local-demo/kstreams Start local kafka, with the 3 apps, MQ and ElasticSearch services run cd local-demo/kstreams docker-compose -f docker-compose-all.yaml up -d As an alternate to only start Kafka, MQ and the 3 apps run: cd local-demo/kstreams docker-compose up -d As another alternate without MQ and elasticSearch: cd local-demo/kstreams docker-compose -f docker-compose-kafka.yaml up -d Demonstration script for local \u00b6 The demonstration script is the same as the one described in this chapter , except that we use Kafdrop to visualize the content of Event Streams topics. Use the simulator the console is: http://localhost:8080/#/ . If you run the controlled scenario the data are: Store Item Action Store 1 Item_1 +10 Store 1 Item_2 +5 Store 1 Item_3 +15 Store 2 Item_1 +10 Store 3 Item_1 +10 Store 4 Item_1 +10 Store 5 Item_1 +10 Store 1 Item_2 -5 Store 1 Item_3 -5 Inventory should be at the store level: {\"stock\":{\"Item_3\":10,\"Item_2\":0,\"Item_1\":10},\"storeName\":\"Store_1\"} and at the item level: Item Stock Item_1 50 Item_2 0 Item_3 10 The store inventory API is at http://localhost:8082 The item inventory API is at http://localhost:8081 Kafdrop UI to see messages in items , store.inventory and item.inventory topics is at http://localhost:9000 Verify Events are in items topic using Kafdrop: Verify item inventory events are in item-inventory Finally verify item inventory events are in store-inventory If using ElasticSearch go to Kibana UI at localhost:5601 Stop the demo: select one of the following command: docker-compose -f docker-compose-all.yaml down # OR docker-compose down # OR docker-compose -f docker-compose-kafka.yaml down","title":"Introduction"},{"location":"#real-time-inventory-demonstration","text":"This repository uses OpenShift GitOps to manage the deployment of the real-time inventory demonstration.","title":"Real-time inventory demonstration"},{"location":"#what-you-will-learn","text":"Use Quarkus, with reactive programming API like Mutiny, and Kafka API to produce messages to Kafka Same Quarkus app can generate messages to RabbitMQ using the AMQP API Same Quarkus app can generate messages to IBM MQ using JMS Use Quarkus and Kafka Streams to compute aggregates to build an inventory view from the stream of sale order events Use IBM Event Streams, Kafka Connector Use the RabbitMQ source connector from IBM Event messaging open source contribution Use the IBM MQ source connector from IBM Event messaging open source contribution","title":"What you will learn"},{"location":"#why-to-consider","text":"This project can be a good foundation to discuss GitOps deployment, and reuse scripts, makefile... to deploy any event-driven solution. As a developer you will use Microprofile reactive messaging, Kafka Streams API, Quarkus and OpenLiberty. Kafka IBM MQ source connector from IBM Event messaging github, Elastic search sink connector...","title":"Why to consider"},{"location":"#use-case-overview","text":"Today, a lot of companies which are managing item / product inventory are facing real challenges to get a close to real-time view of item availability and global inventory view. The solution can be very complex to implement while integrating Enterprise Resource Planning products and other custom legacy systems. Any new solutions are adopting events as a source to exchange data, to put less pressure on existing ERP servers, and to get better visibility into inventory positions while bringing agility to develop new solution with streaming components. This scenario implements a simple near real-time inventory management solution based on real life MVPs we developed in 2020 for different customers. The demonstration illustrates how event streaming processing help to build inventory views. A production deployment will implement different level of store and warehouse inventory aggregators that will push results as events to an event backbone, which in our case is IBM Event Streams. Servers in the Store are sending sale transactions to a central messaging platform, where streaming components are computing the different aggregates. This is a classical data streaming pipeline. Sink connectors, based on Kafka Connect framework, may be used to move data to long persistence storage like object storage or a Database, to integrate results back to Legacy ERP, to use indexing product like Elastic Search, or to propagate events to dashboards. In real life, an as-is solution will include back-end applications to manage the warehouses inventory, connected to a home-built fulfillment application, combined with store applications and servers, e-commerce suite, and a set of SOA services exposing backend systems. This is the larger view of the following figure: We may have integration flows to do data mapping, but most of those calls are synchronous. To get item availability, a lot of SOAP calls are done, increasing latency, and the risk of failure. There is an interesting video from Scott Havens explaining the needs from transitioning from a synchronous architecture to an event-driven asynchronous architecture when scaling, and low latency are must have. This lab reflects this approach.","title":"Use Case Overview"},{"location":"#demonstration-components","text":"The demonstration may be used with different level of integration, and you can select those a la carte . The following diagram illustrates the current components: Diagram source: rt-inventory diagram The store simulator application is a Quarkus based microservice, used to generate item sales to different possible messaging middlewares ( RabbitMQ, IBM MQ or directly to IBM Event Streams). If you want to browse the code, the main readme of this project includes how to package and run the app with docker compose. A code explanation section may give you some ideas to developers. The docker image is quay.io/ibmcase/eda-store-simulator/ and can be used for demonstration. The item inventory aggregator is a stateful application, done with Kafka Stream API. The source code is in the refarch-eda-item-inventory project . Consider this more as a black box in the context of the scenario, it consumes items events, aggregate them, expose APIs on top of Kafka Streams interactive queries and publishes inventory events on item.inventory topic. As a developer you may want to understand the Kafka Stream programming with the following labs , and then considering looking at the classes: ItemProcessingAgent.java . The store inventory aggregator is a Kafka Stream application, also done with Kafka Stream API. The source code is in the refarch-eda-store-inventory project . The output is in store.inventory topic. When messages are sourced to Queues, then a Kafka Source Connector is used to propagate message to items topics. The MQ to Kafka Kafka connect cluster is defined in the eda-rt-inventory-GitOps repository under the kconnect folder, and the source connector in environments/rt-inventory-dev/apps/mq-source The Kafka to Cloud Object Storage Kafka (S3 bucket) connector is defined in the environments/rt-inventory-dev/apps/cos-sink folder . The Sink connector to Elastic Search is defined in environments/rt-inventory-dev/apps/elastic-sink folder . Kafka Connect is used to integrate external systems into Kafka. For example external systems can inject item sale messages to queue, from which a first MQ source Kafka connector publishes the messages to the items Kafka topic. Items sold events are processed by a series of streaming microservices which publishes aggregation results to different topics. Those topics content could be which will be used by Sink connectors to send records to other external systems.","title":"Demonstration components"},{"location":"#general-pre-requisites","text":"Get access to an OpenShift Cluster. All the CLI commands must be performed by a Cluster administrator. You need oc cli and the jq JSON stream editor installed. OpenShift CLI on your local environment. jq on your local environment. Docker and docker compose to run the solution locally. git CLI . Clone this repository git clone https://github.com/ibm-cloud-architecture/eda-rt-inventory-gitops.git For OpenShift deployment, you need access to a cluster with storage capabilities to support Event Streams deployments like block storage configured to use the XFS or ext4 file system, as described in Event Streams storage . You need to have at least one volume per broker and one per zookeeper instance. See also the interactive Installation Guide for cloud pak for integration .","title":"General pre-requisites"},{"location":"#a-gitops-approach-for-solution-deployment","text":"As any cloud-native and kubernetes based solution, we use continuous integration and continuous deployment practices. From a demonstration point of view, the most interesting part is to execute continuous deployment using a GitOps approach as presented in the EDA reference. See the specific explanation in this section.","title":"A GitOps approach for solution deployment"},{"location":"#choose-a-runtime-option","text":"We try to make the business scenario, easily demonstrable by enablind developer's laptop execution with docker compose or use a simple free OpenShift Cluster on IBM cloud. Run on your laptop Use GitOps on a new OpenShift Cluster Use Gitops on existing Cloud Pak Integration (multi-tenant) Without GitOps, just yaml, on a new OpenShift Cluster Without GitOps, just yaml, on existing Cloud Pak Integration (multi-tenant)","title":"Choose a runtime option"},{"location":"#a-non-gitops-approach","text":"","title":"A non gitops approach"},{"location":"#deploy-on-a-brand-new-openshift-cluster","text":"The makefile in this repository supports the minimum commands to use to deploy the different components: # [optional]: prepare entitlementkey, IBM catalog make prepare # [optional]: install the different cp4i operators make install_cp4i_operators # Deploy the dev environment make deploy_rt_inventory","title":"Deploy on a brand new OpenShift cluster"},{"location":"#deploy-on-multi-tenant-environment","text":"The same makefile supports also to deploy to an existing Cloud Pak for Integration deployment with Event Streams being part of a namespace named cp4i-eventstreams : # Deploy all apps in rt-inventory-dev but use cp4i-eventstreams make multi_tenants","title":"Deploy on multi-tenant environment"},{"location":"#two-different-streaming-approaches","text":"We propose two approaches to develop the streaming processing. One using Kafka Streams with two applications One using Apache Flink","title":"Two different streaming approaches"},{"location":"#kafka-streams-implementation","text":"The Item-aggregator, based on Kafka Stream APIs, is in this project: refarch-eda-item-inventory The Store-aggregator, also based on Kafka Stream APIs, is in this project: refarch-eda-store-inventory","title":"Kafka Streams implementation"},{"location":"#fink-implementation","text":"See the refarch-eda-item-inventory-sql-flink repository for more information.","title":"Fink implementation"},{"location":"#run-the-solution-locally","text":"As a developer or technical seller, you could demonstrate this scenario on your laptop using MQ and Event Streams docker images. The docker images for each custom microservices used in this solution are in public registry ( Quay.io ). Under this repository the local-demo/kstream folder has different docker compose files to run different components: docker-compose.yaml for Event Streams, IBM MQ, Kafka Connector the Store Simulator App, the Item aggregator App, the Store aggregator App, and KafDrop to get a user interface to Kafka. docker-compose-all.yaml same as above plus ElasticSearch (1 node) and Kibana Once you have cloned the gitops repository (see pre-requisites section), go under the local-demo/kstreams Start local kafka, with the 3 apps, MQ and ElasticSearch services run cd local-demo/kstreams docker-compose -f docker-compose-all.yaml up -d As an alternate to only start Kafka, MQ and the 3 apps run: cd local-demo/kstreams docker-compose up -d As another alternate without MQ and elasticSearch: cd local-demo/kstreams docker-compose -f docker-compose-kafka.yaml up -d","title":"Run the solution locally"},{"location":"#demonstration-script-for-local","text":"The demonstration script is the same as the one described in this chapter , except that we use Kafdrop to visualize the content of Event Streams topics. Use the simulator the console is: http://localhost:8080/#/ . If you run the controlled scenario the data are: Store Item Action Store 1 Item_1 +10 Store 1 Item_2 +5 Store 1 Item_3 +15 Store 2 Item_1 +10 Store 3 Item_1 +10 Store 4 Item_1 +10 Store 5 Item_1 +10 Store 1 Item_2 -5 Store 1 Item_3 -5 Inventory should be at the store level: {\"stock\":{\"Item_3\":10,\"Item_2\":0,\"Item_1\":10},\"storeName\":\"Store_1\"} and at the item level: Item Stock Item_1 50 Item_2 0 Item_3 10 The store inventory API is at http://localhost:8082 The item inventory API is at http://localhost:8081 Kafdrop UI to see messages in items , store.inventory and item.inventory topics is at http://localhost:9000 Verify Events are in items topic using Kafdrop: Verify item inventory events are in item-inventory Finally verify item inventory events are in store-inventory If using ElasticSearch go to Kibana UI at localhost:5601 Stop the demo: select one of the following command: docker-compose -f docker-compose-all.yaml down # OR docker-compose down # OR docker-compose -f docker-compose-kafka.yaml down","title":"Demonstration script for local"},{"location":"demo-script/","text":"How to demonstrate the real time inventory scenario \u00b6 Real-time inventory scenario presentation \u00b6 Stores are sending their sale transactions to a central messaging platform, based on IBM MQ queues and Kafka topics. As illustrated by the following figure, we are using Kafka / Event Streams to support the events pub/sub and the need to have aggregators to compute store inventory and item cross stores inventory. The following figure illustrates the expected components deployed by this GitOps: The store simulator sends sell or restock messages to MQ ITEMS queue, which are picked up by Kafka MQ source connector to publish to kafka items topic. As an alternate demo, the store simulator may send directly to Kafka to the items topic The Item-aggregator component computes items inventory cross stores, so aggregate at the item_ID level. The Store-aggregator computes aggregates at the store level for each items. Sink connector can write to Cloud Object Storage buckets Sink connector can write to ElasticSearch Warning As of now, the sink connectors to ElasticSearch and COS are not configured in the standard demonstration. Demonstration script \u00b6 Use store simulator \u00b6 Get the Store simulator route using the following command and start a Web Browser chrome http:// $( oc get routes store-simulator -o jsonpath = \"{.spec.host}\" ) You should reach the Home page of the simulator - (Version number may be different) The simulator will send different sell events for the stores as listed in the Stores table. The list of stores is as of now static, and does not have any special semantic, except the store ID that will be used in the generated messages. Look at existing stores, using the top right STORES menu. This is just for viewing the data. No action can be done on this screen. Go to the SIMULATOR tab, and start the controlled scenario which will send the following predefined records: Here is the ordered list of messages sent: Store Item Action Store 1 Item_1 +10 Store 1 Item_2 +5 Store 1 Item_3 +15 Store 2 Item_1 +10 Store 3 Item_1 +10 Store 4 Item_1 +10 Store 5 Item_1 +10 Store 1 Item_2 -5 Store 1 Item_3 -5 once started a table should be displayed to present the records sent to MQ (or Kafka). Verify MQ queue \u00b6 Info The deployment of the solution has created a MQ source connector and a Kafka Connector cluster. It is possible to start and stop the MQ source connector with the following command from your laptop: # Stop the connector oc delete -f environments/multi-tenancy/apps/mq-source/kafka-mq-src-connector.yaml # Start oc apply -f environments/multi-tenancy/apps/mq-source/kafka-mq-src-connector.yaml If Kafka connector is not running, you will be able to verify the messages are in MQ queue - Access the MQ Console using the Cloud Pak for integration navigator and messaging: Go to an URL like: https://cpd-cp4i.apps.rey.coc-ibm.com/zen/#/homepage Select Messaging on the left menu and store-mq instance Go to the the Queue manager admin console and select the QM1 Queue manager Select the ITEMS queue to verify the messages reach the queue. It may be possible that the Kafka Connector already consumed those messages so the queue may look empty. Below is a view of one of those message. Verify messages in Event Streams \u00b6 If the Kafka MQ source connector is running then the messages published to MQ are immediatly processed and sent to the items topic. As the two aggregator applications are also processing the item events, you should get messages in the item.inventory topic: and the store.inventory You can also verify the current consumer groups subscribing to the items topic: Using interactive queries \u00b6 Let assess if we can see the item stock cross stores: using the item-aggregator route, something like item-aggregator-rt-inventory.....us-east.containers.appdomain.cloud but completed with '/q/swagger-ui' as we want to access the API To get this route use the following command: chrome http:// $( oc get routes item-inventory -o jsonpath = \"{.spec.host}\" ) /q/swagger-ui Select the get /api/v1/items/{itemID} operation: Use one of the following item id: [Item_1, Item_2, Item_3, Item_4, Item_5, Item_6, Item_7]. You should get the current stock cross stores Let assess a store stock, for that we access the store aggregator URL: store-aggregator-rt-inventory.dte-ocp46-73awfj-915b3b336cabec458a7c7ec2aa7c625f-0000.us-east.containers.appdomain.cloud with the /q/swagger-ui suffix. To get this route use the following command: chrome http:// $( oc get routes store-aggregator -o jsonpath = \"{.spec.host}\" ) /q/swagger-ui Then using the GET on the api/v1/stores/inventory/{storeID} , and enter one of the available store: [Store_1, Store_2, Store_3, Store_4, Store_5] Continuous message generation \u00b6 Using the simulator it is possible to send n messages with random payload or send continuously by selecting one of the options:","title":"Demonstration script"},{"location":"demo-script/#how-to-demonstrate-the-real-time-inventory-scenario","text":"","title":"How to demonstrate the real time inventory scenario"},{"location":"demo-script/#real-time-inventory-scenario-presentation","text":"Stores are sending their sale transactions to a central messaging platform, based on IBM MQ queues and Kafka topics. As illustrated by the following figure, we are using Kafka / Event Streams to support the events pub/sub and the need to have aggregators to compute store inventory and item cross stores inventory. The following figure illustrates the expected components deployed by this GitOps: The store simulator sends sell or restock messages to MQ ITEMS queue, which are picked up by Kafka MQ source connector to publish to kafka items topic. As an alternate demo, the store simulator may send directly to Kafka to the items topic The Item-aggregator component computes items inventory cross stores, so aggregate at the item_ID level. The Store-aggregator computes aggregates at the store level for each items. Sink connector can write to Cloud Object Storage buckets Sink connector can write to ElasticSearch Warning As of now, the sink connectors to ElasticSearch and COS are not configured in the standard demonstration.","title":"Real-time inventory scenario presentation"},{"location":"demo-script/#demonstration-script","text":"","title":"Demonstration script"},{"location":"demo-script/#use-store-simulator","text":"Get the Store simulator route using the following command and start a Web Browser chrome http:// $( oc get routes store-simulator -o jsonpath = \"{.spec.host}\" ) You should reach the Home page of the simulator - (Version number may be different) The simulator will send different sell events for the stores as listed in the Stores table. The list of stores is as of now static, and does not have any special semantic, except the store ID that will be used in the generated messages. Look at existing stores, using the top right STORES menu. This is just for viewing the data. No action can be done on this screen. Go to the SIMULATOR tab, and start the controlled scenario which will send the following predefined records: Here is the ordered list of messages sent: Store Item Action Store 1 Item_1 +10 Store 1 Item_2 +5 Store 1 Item_3 +15 Store 2 Item_1 +10 Store 3 Item_1 +10 Store 4 Item_1 +10 Store 5 Item_1 +10 Store 1 Item_2 -5 Store 1 Item_3 -5 once started a table should be displayed to present the records sent to MQ (or Kafka).","title":"Use store simulator"},{"location":"demo-script/#verify-mq-queue","text":"Info The deployment of the solution has created a MQ source connector and a Kafka Connector cluster. It is possible to start and stop the MQ source connector with the following command from your laptop: # Stop the connector oc delete -f environments/multi-tenancy/apps/mq-source/kafka-mq-src-connector.yaml # Start oc apply -f environments/multi-tenancy/apps/mq-source/kafka-mq-src-connector.yaml If Kafka connector is not running, you will be able to verify the messages are in MQ queue - Access the MQ Console using the Cloud Pak for integration navigator and messaging: Go to an URL like: https://cpd-cp4i.apps.rey.coc-ibm.com/zen/#/homepage Select Messaging on the left menu and store-mq instance Go to the the Queue manager admin console and select the QM1 Queue manager Select the ITEMS queue to verify the messages reach the queue. It may be possible that the Kafka Connector already consumed those messages so the queue may look empty. Below is a view of one of those message.","title":"Verify MQ queue"},{"location":"demo-script/#verify-messages-in-event-streams","text":"If the Kafka MQ source connector is running then the messages published to MQ are immediatly processed and sent to the items topic. As the two aggregator applications are also processing the item events, you should get messages in the item.inventory topic: and the store.inventory You can also verify the current consumer groups subscribing to the items topic:","title":"Verify messages in Event Streams"},{"location":"demo-script/#using-interactive-queries","text":"Let assess if we can see the item stock cross stores: using the item-aggregator route, something like item-aggregator-rt-inventory.....us-east.containers.appdomain.cloud but completed with '/q/swagger-ui' as we want to access the API To get this route use the following command: chrome http:// $( oc get routes item-inventory -o jsonpath = \"{.spec.host}\" ) /q/swagger-ui Select the get /api/v1/items/{itemID} operation: Use one of the following item id: [Item_1, Item_2, Item_3, Item_4, Item_5, Item_6, Item_7]. You should get the current stock cross stores Let assess a store stock, for that we access the store aggregator URL: store-aggregator-rt-inventory.dte-ocp46-73awfj-915b3b336cabec458a7c7ec2aa7c625f-0000.us-east.containers.appdomain.cloud with the /q/swagger-ui suffix. To get this route use the following command: chrome http:// $( oc get routes store-aggregator -o jsonpath = \"{.spec.host}\" ) /q/swagger-ui Then using the GET on the api/v1/stores/inventory/{storeID} , and enter one of the available store: [Store_1, Store_2, Store_3, Store_4, Store_5]","title":"Using interactive queries"},{"location":"demo-script/#continuous-message-generation","text":"Using the simulator it is possible to send n messages with random payload or send continuously by selecting one of the options:","title":"Continuous message generation"},{"location":"gitops/","text":"GitOps approach \u00b6 What is covered \u00b6 This GitOps supports bootstrapping the solution as a Day 1 operation, with the deployment of operators, secrets, pipelines... Then with Day 2 operations any changes to the configurations done in this repository, managed with the Git PR process, are propagated by ArgoCD to the runtime cluster. The GitOps approach is an adaptation of Red Hat's KAM practices enhanced to be able to boostrap some important operators like the OpenShift GitOps Operator and OpenShift Pipelines Operator and Cloud Pak for integration operators. kam bootstrap \\ --service-repo-url https://github.com/ibm-cloud-architecture/refarch-eda-store-inventory \\ --gitops-repo-url https://github.com/ibm-cloud-architecture/eda-rt-inventory-gitops \\ --image-repo image-registry.openshift-image-registry.svc:5000/ibmcase/ \\ --output eda-rt-inventory-gitops \\ --git-host-access-token <a-github-token> \\ --prefix edademo --push-to-git = true The generated content was enhanced to add boostraping configuration and scripts, the final repository structure includes: Boostrap folder: to install different operators and to define the ArgoCD project named rt-inventory . config folder, is for defining the ArgoCD apps and the app of apps. kconnect folder is used to build a custom docker image of Kafka connector with MQ source, Elasticsearch sink and Cloud Object storage sink. local-demo folder is for running the solution on your laptop using docker-compose. environments folder, is the most important one, it uses Kustomize to declare environments (dev, staging) and component deployments (See next section for details). We also added a Makefile and scripts to deploy the gitops, pipelines operators and different elements with or without GitOps. In this Gitops you can use different approaches to deploy the real-time inventory solution depending of your existing environment. Start from an OpenShift Cluster without any Cloud Pak for Integration components , this will take few hours to deploy as some Operator and Operand deployments may take time. Start from a Cloud Pak for integration deployed in cp4i-* projects GitOps on new OpenShift Cluster \u00b6 This GitOps repository (represented as the yellow rectangle in figure below) defines the ArgoCD apps used to monitor and deploy the different microservices, streaming processing apps, and the different IBM products needed: Event Streams, MQ, API management, event-end-point management. The figure belows presents the adopted strategy: The gitops catalog repository , represented with a blue rectangle, defines the different operator subscriptions for the IBM Cloud Pak for Integration components. Centralizing to one repository such operator subscriptions enforces reuse between solutions. What is deployed in this demonstration \u00b6 The development project includes event-streams, MQ, schema registry... The installation approach is to deploy operators to manage All Namespaces, at the cluster scope. So only one Platform UI can be installed per cluster. A single instance of IBM Cloud Pak foundational services is installed in the ibm-common-services namespace. The following operators may be installed from this GitOps: ibm-integration-platform-navigator ibm-integration-asset-repository ibm-integration-operations-dashboard ibm-eventstreams ibm-mq The entitlement key secret will be copied to each namespace where some of the Cloud Pak integration products are deployed, using a kubernetes job. Part of this deployment will be based on commands run from your laptop, part as pipelines, and part as ArgoCD apps. The approach is based on the following: secrets, and operators deployments to bootstrap the CI/CD are configured with Makefile and commands. Operators are deployed in openshift-operators . Tekton pipelines are used to deploy some CP4I operators ArgoCD apps are used to deploy CP4I operands: the use of ArgoCD for this, is justified for Day 2 operations. The pipelines are using a service account, named pipeline , in the rt-inventory-cicd project, and cluster role to access different resources cross namespaces. CP4Integration installation considerations \u00b6 In this solution, CP4I operators are deployed in All namespaces , the entire OpenShift cluster effectively behaves as one large tenant. With All namespace there can be only one Platform Navigator installed per cluster, and all Cloud Pak instances are owned by that Platform Navigator. A single instance of IBM Cloud Pak foundational services is installed in the ibm-common-services namespace if the foundational services operator is not already installed on the cluster. Operators can be upgraded automatically when new compatible versions are available. For production deployment, the manual upgrade may be desirable. Bootstrap GitOps \u00b6 The current GitOps was run on OpenShift 4.8. Login to the OpenShift Console, and get login token to be able to use oc cli Obtain your IBM license entitlement key and export as KEY environment variables export KEY = <yourentitlementkey> create github-credentials.yaml file for the git secret based on template-github-credentials.yaml . Use your github personal access token. It will be used by the pipeline runs. create a Secret for your IBM Cloud Object Storage credential. Use the on template-cos-credentials.yaml and modify the following parameters: cos.api.key : <cos-credential.field.apikey> cos.bucket.location : <region where the cos bucket is> cos.bucket.name : <bucketname> cos.service.crn : <cos-credential.field.iam_serviceid_crn> If not done already, use the following command to install GitOps and Pipeline operators, entitlement key, and IBM image catalog: make prepare Once the operators are running the command: oc get pods -n openshift-gitops should return a list of pods like: NAME READY STATUS RESTARTS AGE openshift-gitops-application-controller-0 1 /1 Running 0 4h5m openshift-gitops-applicationset-controller-6948bcf87c-jdv2x 1 /1 Running 0 4h5m openshift-gitops-dex-server-64cbd8d7bd-76czz 1 /1 Running 0 4h5m openshift-gitops-redis-7867d74fb4-dssr2 1 /1 Running 0 4h5m openshift-gitops-repo-server-6dc777c845-gdjhr 1 /1 Running 0 4h5m openshift-gitops-server-7957cc47d9-cmxvw 1 /1 Running 0 4h5m Deploy different IBM product Operators (Event Streams, MQ...) to monitor All Namespaces : make install_cp4i_operators The IBM common services deployment can take more than 30 minutes. Get the ArgoCD User Interface URL and open a web browser: chrome https:// $( oc get route openshift-gitops-server -o jsonpath = '{.status.ingress[].host}' -n openshift-gitops ) Deploy ArgoCD app of apps: \u00b6 To start the Continuous Deployment with ArgoCD, just executing the following command should deploy event streams cluster instance, MQ broker, kafka connect, and the different microservices. oc apply -k config/argocd # Or make start_argocd_apps The expected set of ArgoCD apps looks like: rt-inventory-Argo-app is an app of apps rt-inventory-dev-env is for the rt-inventory-dev namespace rt-inventory-dev-services is for event streams, kafka connect cluster and mq deployments in dev-env namespace rt-inventory-store-simulator-app is for the simulator app used in the demo. rt-inventory-item-inventory for the item aggregator application rt-inventory-store-inventory for the store aggregator application Potential errors \u00b6 \"ConfigMap ibm-common-services-status in kube-public to be ready\" While the Event Streams cluster is created: An unexpected exception was encountered: Exceeded timeout of 1200000ms while waiting for ConfigMap resource ibm-common-services-status in namespace kube-public to be ready. More detail can be found in the Event Streams Operator log. This is an issue known as of 10.5. Restart the ES operator pod See also https://github.ibm.com/mhub/qp-planning/issues/7383 Configure connector \u00b6 Go to the dev project: oc project rt-inventory-dev Deploy the sink kafka connector for cloud object storage: Modify the file kafka-cos-sink-connector.yaml in environments/rt-inventory-dev/apps/cos-sink , by replacing the following line from the cloud object storage credentials: cos.api.key : IBM_COS_API_KEY cos.bucket.location : IBM_COS_BUCKET_LOCATION cos.bucket.name : IBM_COS_BUCKET_NAME cos.bucket.resiliency : IBM_COS_RESILIENCY cos.service.crn : \"IBM_COS_CRM\" Then deploy the connector: oc apply -f environments/rt-inventory-dev/apps/cos-sink/kafka-cos-sink-connector.yaml Deploy the MQ source connector oc apply -f environments/rt-inventory-dev/apps/mq-source/kafka-mq-src-connector.json Access to the Simulator User Interface via: chrome http:// $( oc get route store-simulator -o jsonpath = '{.status.ingress[].host}' ) Access Event Stream Console: chrome https:// $( oc get route dev-ibm-es-ui -o jsonpath = '{.status.ingress[].host}' ) Access to IBM MQ Admin Console chrome https:// $( oc get route store-mq-ibm-mq-qm -o jsonpath = '{.status.ingress[].host}' ) Deploy in an existing CP4I deployment \u00b6 In this section we suppose CP4I is already deployed in cp4i namespace, event streams in cp4i-eventstreams project. So somewhere someone has already deployed the infrastructure, and other components as multi tenants. (This is represented as the green rectangles in the figure below) Some particularities: Event Streams is in its own project, so kafka topics, kafka users has to follow a naming convention to avoid colision with other teams / solutions. MQ broker runs local to the solution namespace. ( rt-inventory-dev has its own MQ Broker) The command below will not use GitOpa / ArgoCD make multi-tenants Get Store Simulator URL and execute the demonstration script: chrome $( oc get routes store-simulator -o jsonpath = ' { .status.ingress [] .host } ; ) Bootstrap GitOps \u00b6 Login to the OpenShift Console, and get login token to be able to use oc cli If not done already, use the script to install GitOps and Pipeline operators: make verify_argocd Create an ArgoCD project named rt-inventory oc apply -k bootstrap/argocd-project # Result appproject.argoproj.io/rt-inventory created To get the admin user's password use the command oc extract secret/openshift-gitops-cluster -n openshift-gitops --to = - Get the ArgoCD User Interface URL and open a web browser chrome https:// $( oc get route openshift-gitops-server -o jsonpath = '{.status.ingress[].host}' -n openshift-gitops ) Deploy the solution \u00b6 To start the Continuous Deployment with ArgoCD, just executing the following command should deploy different microservices under rt-inventory-dev project using event-streams, MQ.. from another project (e.g. cp4i). oc apply -k config/cp4i-deploy","title":"GitOps approach"},{"location":"gitops/#gitops-approach","text":"","title":"GitOps approach"},{"location":"gitops/#what-is-covered","text":"This GitOps supports bootstrapping the solution as a Day 1 operation, with the deployment of operators, secrets, pipelines... Then with Day 2 operations any changes to the configurations done in this repository, managed with the Git PR process, are propagated by ArgoCD to the runtime cluster. The GitOps approach is an adaptation of Red Hat's KAM practices enhanced to be able to boostrap some important operators like the OpenShift GitOps Operator and OpenShift Pipelines Operator and Cloud Pak for integration operators. kam bootstrap \\ --service-repo-url https://github.com/ibm-cloud-architecture/refarch-eda-store-inventory \\ --gitops-repo-url https://github.com/ibm-cloud-architecture/eda-rt-inventory-gitops \\ --image-repo image-registry.openshift-image-registry.svc:5000/ibmcase/ \\ --output eda-rt-inventory-gitops \\ --git-host-access-token <a-github-token> \\ --prefix edademo --push-to-git = true The generated content was enhanced to add boostraping configuration and scripts, the final repository structure includes: Boostrap folder: to install different operators and to define the ArgoCD project named rt-inventory . config folder, is for defining the ArgoCD apps and the app of apps. kconnect folder is used to build a custom docker image of Kafka connector with MQ source, Elasticsearch sink and Cloud Object storage sink. local-demo folder is for running the solution on your laptop using docker-compose. environments folder, is the most important one, it uses Kustomize to declare environments (dev, staging) and component deployments (See next section for details). We also added a Makefile and scripts to deploy the gitops, pipelines operators and different elements with or without GitOps. In this Gitops you can use different approaches to deploy the real-time inventory solution depending of your existing environment. Start from an OpenShift Cluster without any Cloud Pak for Integration components , this will take few hours to deploy as some Operator and Operand deployments may take time. Start from a Cloud Pak for integration deployed in cp4i-* projects","title":"What is covered"},{"location":"gitops/#gitops-on-new-openshift-cluster","text":"This GitOps repository (represented as the yellow rectangle in figure below) defines the ArgoCD apps used to monitor and deploy the different microservices, streaming processing apps, and the different IBM products needed: Event Streams, MQ, API management, event-end-point management. The figure belows presents the adopted strategy: The gitops catalog repository , represented with a blue rectangle, defines the different operator subscriptions for the IBM Cloud Pak for Integration components. Centralizing to one repository such operator subscriptions enforces reuse between solutions.","title":"GitOps on new OpenShift Cluster"},{"location":"gitops/#what-is-deployed-in-this-demonstration","text":"The development project includes event-streams, MQ, schema registry... The installation approach is to deploy operators to manage All Namespaces, at the cluster scope. So only one Platform UI can be installed per cluster. A single instance of IBM Cloud Pak foundational services is installed in the ibm-common-services namespace. The following operators may be installed from this GitOps: ibm-integration-platform-navigator ibm-integration-asset-repository ibm-integration-operations-dashboard ibm-eventstreams ibm-mq The entitlement key secret will be copied to each namespace where some of the Cloud Pak integration products are deployed, using a kubernetes job. Part of this deployment will be based on commands run from your laptop, part as pipelines, and part as ArgoCD apps. The approach is based on the following: secrets, and operators deployments to bootstrap the CI/CD are configured with Makefile and commands. Operators are deployed in openshift-operators . Tekton pipelines are used to deploy some CP4I operators ArgoCD apps are used to deploy CP4I operands: the use of ArgoCD for this, is justified for Day 2 operations. The pipelines are using a service account, named pipeline , in the rt-inventory-cicd project, and cluster role to access different resources cross namespaces.","title":"What is deployed in this demonstration"},{"location":"gitops/#cp4integration-installation-considerations","text":"In this solution, CP4I operators are deployed in All namespaces , the entire OpenShift cluster effectively behaves as one large tenant. With All namespace there can be only one Platform Navigator installed per cluster, and all Cloud Pak instances are owned by that Platform Navigator. A single instance of IBM Cloud Pak foundational services is installed in the ibm-common-services namespace if the foundational services operator is not already installed on the cluster. Operators can be upgraded automatically when new compatible versions are available. For production deployment, the manual upgrade may be desirable.","title":"CP4Integration installation considerations"},{"location":"gitops/#bootstrap-gitops","text":"The current GitOps was run on OpenShift 4.8. Login to the OpenShift Console, and get login token to be able to use oc cli Obtain your IBM license entitlement key and export as KEY environment variables export KEY = <yourentitlementkey> create github-credentials.yaml file for the git secret based on template-github-credentials.yaml . Use your github personal access token. It will be used by the pipeline runs. create a Secret for your IBM Cloud Object Storage credential. Use the on template-cos-credentials.yaml and modify the following parameters: cos.api.key : <cos-credential.field.apikey> cos.bucket.location : <region where the cos bucket is> cos.bucket.name : <bucketname> cos.service.crn : <cos-credential.field.iam_serviceid_crn> If not done already, use the following command to install GitOps and Pipeline operators, entitlement key, and IBM image catalog: make prepare Once the operators are running the command: oc get pods -n openshift-gitops should return a list of pods like: NAME READY STATUS RESTARTS AGE openshift-gitops-application-controller-0 1 /1 Running 0 4h5m openshift-gitops-applicationset-controller-6948bcf87c-jdv2x 1 /1 Running 0 4h5m openshift-gitops-dex-server-64cbd8d7bd-76czz 1 /1 Running 0 4h5m openshift-gitops-redis-7867d74fb4-dssr2 1 /1 Running 0 4h5m openshift-gitops-repo-server-6dc777c845-gdjhr 1 /1 Running 0 4h5m openshift-gitops-server-7957cc47d9-cmxvw 1 /1 Running 0 4h5m Deploy different IBM product Operators (Event Streams, MQ...) to monitor All Namespaces : make install_cp4i_operators The IBM common services deployment can take more than 30 minutes. Get the ArgoCD User Interface URL and open a web browser: chrome https:// $( oc get route openshift-gitops-server -o jsonpath = '{.status.ingress[].host}' -n openshift-gitops )","title":"Bootstrap GitOps"},{"location":"gitops/#deploy-argocd-app-of-apps","text":"To start the Continuous Deployment with ArgoCD, just executing the following command should deploy event streams cluster instance, MQ broker, kafka connect, and the different microservices. oc apply -k config/argocd # Or make start_argocd_apps The expected set of ArgoCD apps looks like: rt-inventory-Argo-app is an app of apps rt-inventory-dev-env is for the rt-inventory-dev namespace rt-inventory-dev-services is for event streams, kafka connect cluster and mq deployments in dev-env namespace rt-inventory-store-simulator-app is for the simulator app used in the demo. rt-inventory-item-inventory for the item aggregator application rt-inventory-store-inventory for the store aggregator application","title":"Deploy ArgoCD app of apps:"},{"location":"gitops/#potential-errors","text":"\"ConfigMap ibm-common-services-status in kube-public to be ready\" While the Event Streams cluster is created: An unexpected exception was encountered: Exceeded timeout of 1200000ms while waiting for ConfigMap resource ibm-common-services-status in namespace kube-public to be ready. More detail can be found in the Event Streams Operator log. This is an issue known as of 10.5. Restart the ES operator pod See also https://github.ibm.com/mhub/qp-planning/issues/7383","title":"Potential errors"},{"location":"gitops/#configure-connector","text":"Go to the dev project: oc project rt-inventory-dev Deploy the sink kafka connector for cloud object storage: Modify the file kafka-cos-sink-connector.yaml in environments/rt-inventory-dev/apps/cos-sink , by replacing the following line from the cloud object storage credentials: cos.api.key : IBM_COS_API_KEY cos.bucket.location : IBM_COS_BUCKET_LOCATION cos.bucket.name : IBM_COS_BUCKET_NAME cos.bucket.resiliency : IBM_COS_RESILIENCY cos.service.crn : \"IBM_COS_CRM\" Then deploy the connector: oc apply -f environments/rt-inventory-dev/apps/cos-sink/kafka-cos-sink-connector.yaml Deploy the MQ source connector oc apply -f environments/rt-inventory-dev/apps/mq-source/kafka-mq-src-connector.json Access to the Simulator User Interface via: chrome http:// $( oc get route store-simulator -o jsonpath = '{.status.ingress[].host}' ) Access Event Stream Console: chrome https:// $( oc get route dev-ibm-es-ui -o jsonpath = '{.status.ingress[].host}' ) Access to IBM MQ Admin Console chrome https:// $( oc get route store-mq-ibm-mq-qm -o jsonpath = '{.status.ingress[].host}' )","title":"Configure connector"},{"location":"gitops/#deploy-in-an-existing-cp4i-deployment","text":"In this section we suppose CP4I is already deployed in cp4i namespace, event streams in cp4i-eventstreams project. So somewhere someone has already deployed the infrastructure, and other components as multi tenants. (This is represented as the green rectangles in the figure below) Some particularities: Event Streams is in its own project, so kafka topics, kafka users has to follow a naming convention to avoid colision with other teams / solutions. MQ broker runs local to the solution namespace. ( rt-inventory-dev has its own MQ Broker) The command below will not use GitOpa / ArgoCD make multi-tenants Get Store Simulator URL and execute the demonstration script: chrome $( oc get routes store-simulator -o jsonpath = ' { .status.ingress [] .host } ; )","title":"Deploy in an existing CP4I deployment"},{"location":"gitops/#bootstrap-gitops_1","text":"Login to the OpenShift Console, and get login token to be able to use oc cli If not done already, use the script to install GitOps and Pipeline operators: make verify_argocd Create an ArgoCD project named rt-inventory oc apply -k bootstrap/argocd-project # Result appproject.argoproj.io/rt-inventory created To get the admin user's password use the command oc extract secret/openshift-gitops-cluster -n openshift-gitops --to = - Get the ArgoCD User Interface URL and open a web browser chrome https:// $( oc get route openshift-gitops-server -o jsonpath = '{.status.ingress[].host}' -n openshift-gitops )","title":"Bootstrap GitOps"},{"location":"gitops/#deploy-the-solution","text":"To start the Continuous Deployment with ArgoCD, just executing the following command should deploy different microservices under rt-inventory-dev project using event-streams, MQ.. from another project (e.g. cp4i). oc apply -k config/cp4i-deploy","title":"Deploy the solution"},{"location":"mm2/","text":"Mirror Maker 2 \u00b6 Active/Passive Mirroring \u00b6 Overview \u00b6 This demo presents how to leverage Mirror Maker 2 between two Kafka clusters running on OpenShift. It uses two IBM Event Streams instances on both sides and utilizes mirroring feature that ships as part of IBM Event Streams Operator API's. Cluster 1 (Active): This will be our source cluster (Source). In this case it can be a Development environment where consumers and producers are connected. Cluster 2 (Passive): This will be our target cluster (Target). In this case it can be a Staging environment where no consumers or producers are connected. Upon failover, consumers will be connected to the newly promoted cluster (Cluster 2) which will become active. Mirror Maker 2 is a continuous background mirroring process and can be run in its own namespace. However, for the purposes of this demo, it will be created within the destination namespace, in our case rt-inventory-stg and connect to the source Kafka cluser, in our case it is in rt-inventory-dev namespace. Pre-requisites \u00b6 We assume, you have access to one or two separate OpenShift clusters. The OpenShift cluster(s) has/have IBM Event Streams Operator version 3.0.x installed and available for all namespaces on the cluster(s). Steps \u00b6 1- Creating Source (Origin) Kafka Cluster: Using Web Console on the first OpenShift cluster, create a new project named rt-inventory-dev Make sure that this project is selected and create a new EventStreams instance named rt-inventory-dev inside it. Note This cluster represents the Development Environment (Active) Once the EventStreams instance rt-inventory-dev is up and running, access its Admin UI to perform the below sub-tasks: Create SCRAM credentials with username rt-inv-dev-user . This will create rt-inv-dev-user secret. Take a note of the SCRAM username and password. Take a note of the bootstrap Server address. Generate TLS certificates. This will create rt-inventory-dev-cluster-ca-cert secret. Download and save the PEM certificate. You could rename it to es-src-cert.pem . Create Kafka topics items , items.inventory , and store.inventory which will be used to demonstrate replication from source to target. Note The above two created secrets will need to be copied to the target cluster so Mirror Maker 2 can reference them to connect to the source cluster 2- Creating Target (Destination) Kafka Cluster: Using Web Console on the first OpenShift cluster, create a new project named rt-inventory-stg Make sure that this project is selected and create a new EventStreams instance named rt-inventory-stg inside it. Note This cluster represents the Staging Environment (Passive) Once the EventStreams instance rt-inventory-stg is up and running, access its Admin UI to perform the below sub-tasks: Create SCRAM credentials with username rt-inv-stg-user . This will create rt-inv-stg-user secret. Take a note of the SCRAM username and password. Take a note of the bootstrap Server address. Generate TLS certificates. This will create rt-inventory-stg-cluster-ca-cert secret. Download and save the PEM certificate. You could rename it to es-tgt-cert.pem . Note Mirror Maker 2 will reference the above two created secrets to connect to the target cluster 3- Creating MirrorMaker 2 instance: Make sure that rt-inventory-stg project is selected. Create the two sercets generated from Step 1 by copying their yaml configs from rt-inventory-dev project to preserve the names. Using OpenShift Web Console (Administrator perspective), navigate to Operators -> Installed Operators. On the Installed Operators page, click on IBM Event Sreams . On the Avialable API's page, find Kafka Mirror Maker 2 then click Create instance . Select Configure via YAML view to use the yaml file provided in this demo. Change the bootstrapServers address (line 36) to be the address of your Source Kafka bootstrapServer (from Step 1). Change the bootstrapServers address (line 53) to be the address of your Target Kafka bootstrapServer (from Step 2). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 apiVersion : eventstreams.ibm.com/v1beta2 kind : KafkaMirrorMaker2 metadata : name : mm2 namespace : rt-inventory-stg spec : version : 3.0.1 replicas : 1 connectCluster : es-tgt logging : loggers : connect.root.logger.level : INFO type : inline template : pod : metadata : annotations : cloudpakId : c8b82d189e7545f0892db9ef2731b90d productVersion : 11.0.0 productID : 2a79e49111f44ec3acd89608e56138f5 cloudpakName : IBM Cloud Pak for Integration cloudpakVersion : 2022.1.1 productChargedContainers : mm2-mirrormaker2 productCloudpakRatio : '2:1' productName : IBM Event Streams for Non Production eventstreams.production.type : CloudPakForIntegrationNonProduction productMetric : VIRTUAL_PROCESSOR_CORE clusters : - alias : es-src authentication : passwordSecret : password : password secretName : rt-inv-dev-user type : scram-sha-512 username : rt-inv-dev-user bootstrapServers : rt-inventory-dev-kafka-bootstrap-rt-inventory-dev.itzroks-4b4a.us-south.containers.appdomain.cloud:443 config : ssl.endpoint.identification.algorithm : https config.storage.replication.factor : 1 offset.storage.replication.factor : 1 status.storage.replication.factor : 1 tls : trustedCertificates : - certificate : ca.crt secretName : rt-inventory-dev-cluster-ca-cert - alias : es-tgt authentication : passwordSecret : password : password secretName : rt-inv-stg-user type : scram-sha-512 username : rt-inv-stg-user bootstrapServers : rt-inventory-stg-kafka-bootstrap-rt-inventory-stg.itzroks-4b4b.us-south.containers.appdomain.cloud:443 config : ssl.endpoint.identification.algorithm : https config.storage.replication.factor : 3 offset.storage.replication.factor : 3 status.storage.replication.factor : 3 tls : trustedCertificates : - certificate : ca.crt secretName : rt-inventory-stg-cluster-ca-cert mirrors : - sourceCluster : es-src targetCluster : es-tgt sourceConnector : config : # the replication factor that will be used for # all topics created on the \"target\" Kafka cluster replication.factor : 3 # don't try to copy permissions across from the \"origin\" # cluster to the \"target\" cluster sync.topic.acls.enabled : \"false\" replication.policy.separator : \"\" replication.policy.class : \"io.strimzi.kafka.connect.mirror.IdentityReplicationPolicy\" offset-syncs.topic.replication.factor : 3 refresh.topics.interval.seconds : 10 checkpointConnector : config : checkpoints.topic.replication.factor : 3 refresh.groups.interval.seconds : 5 # migrates the consumer group offsets emit.checkpoints.enabled : true sync.group.offsets.enabled : true sync.group.offsets.interval.seconds : 5 emit.checkpoints.interval.seconds : 5 # ensures that consumer group offsets on the \"target\" cluster # are correctly mapped to consumer groups on the \"origin\" cluster replication.policy.class : \"io.strimzi.kafka.connect.mirror.IdentityReplicationPolicy\" replication.policy.separator : \"\" topicsPattern : \"items, store.inventory, items.inventory\" topicsExcludePattern : \".apicurio\" groupsPattern : \".*\" If the same namespaces and SCRAM usernames are used as indicated in the previous steps, no further changes are required. Click Create to apply the YAML changes and create KafkaMirrorMaker2 instance. In few seconds, check the status of KafkaMirrorMaker2 instance from Kafka Mirror Maker 2 Tab. The status should be Condition: Ready . KafkaMirrorMaker2 instance created mm2 will deploy different resources that can be checked from its Resources Tab. You might need to check the created Pod resource log for errors or warnings. Now, the created instance mm2 will start to mirror (replicate) the Kafka topics' events, and offsets from source to the target cluster. Only Kafka topics specified in topicsPattern will be replicated and topics specified in topicsExcludePattern will be excluded. Verification \u00b6 Access the EventStreams instance rt-inventory-stg Admin UI to verify that the replication is working. From the side menu, select Topics to see the list of Kafka topics created on our Target cluster (Staging Environment). You can see that items , items.inventory , and store.inventory Kafka topics were created and events are being replicated. Kafka Topics named mirrormaker2-cluster-xxx are used internally by our KafkaMirrorMaker2 instance to keep track of configs, offsets, and replication status. Click on items topic then visit the Messages Tab to see that the events are being replicated as they arrive to the Source cluster. The next section will demonstrate how to produce sample events to the Source cluster. Producing Events (Source) \u00b6 This section will be used to verify that the replication of messages (Kafka events) is actually happening (live events feed is moving from Source to Target). We will use a simple python script (accompanied with a starter bash script) that connects to the Source cluster and sends a random json payloads to the items topic created in Step 1. The producer script requires Python 3.x with confluent-kafka Kakfa library installed. To install the Kafka library, run: pip install confluent-kafka Perform the following steps to setup the producer sample application: On your local machine, create a new directory named producer . Download and save SendItems.py and sendItems.sh files inside producer directory. Move the Source cluster PEM certificate file es-src-cert.pem to the same directory. Edit the sendItems.sh script to set the environment variables of Source cluster connectivity configs. Change the KAFKA_BROKERS variable to the Source cluster bootstrap address. Change the KAFKA_PWD variable to be the password of rt-inv-dev-user SCRAM user. #!/bin/bash export KAFKA_BROKERS = rt-inventory-dev-kafka-bootstrap-rt-inventory-dev.itzroks-4b4a.us-south.containers.appdomain.cloud:443 export KAFKA_USER = rt-inv-dev-user export KAFKA_PWD = SCRAM-PASSWORD export KAFKA_CERT = es-src-cert.pem export KAFKA_SECURITY_PROTOCOL = SASL_SSL export KAFKA_SASL_MECHANISM = SCRAM-SHA-512 export SOURCE_TOPIC = items python SendItems.py $1 5. Now you can run the shell script and pass the number of events to be sent as an argument. ./sendItems.sh 10 Consuming Events (Target) \u00b6 This section will simulate failover to Target (passive) cluster. MirrorMaker 2 uses checkpointConnector to automatically store consumer group offset checkpoints for consumer groups on the Source Kafka cluster. Each checkpoint maps the last committed offset for each consumer group in the Source cluster to the equivalent offset in the Target cluster. These checkpoints will be used by our Kafka consumer script to start consuming from an offset on the Target cluster that is equivalent to the last committed offset on the Source cluster. The same way we used to setup the Producer application, we need to perform the following steps to setup the Consumer application: On your local machine, create a new directory named consumer . Download and save ReceiveItems.py and receiveItems.sh files inside consumer directory. Move the Target cluster PEM certificate file es-tgt-cert.pem to the same directory. Edit the receiveItems.sh script to set the environment variables of Target cluster connectivity configs. Change the KAFKA_BROKERS variable to the Target cluster bootstrap address. Change the KAFKA_PWD variable to be the password of rt-inv-stg-user SCRAM user. #!/bin/bash export KAFKA_BROKERS = rt-inventory-stg-kafka-bootstrap-rt-inventory-stg.itzroks-4b4b.us-south.containers.appdomain.cloud:443 export KAFKA_USER = rt-inv-stg-user export KAFKA_PWD = SCRAM-PASSWORD export KAFKA_CERT = es-tgt-cert.pem export KAFKA_SECURITY_PROTOCOL = SASL_SSL export KAFKA_SASL_MECHANISM = SCRAM-SHA-512 export SOURCE_TOPIC = items python ReceiveItems.py $1 5. Now you can run the shell script and pass the number of events to be consumed as an argument. ./consumeItems.sh 10 6. You should see the mirrored messages now in items replicated topic in our Target Kafka cluster. Note To check the current and offset lag, you can use Event Streams UI on Target cluster. We can see that the consumer application has started to read from the last committed offset. This concludes our Kafka Mirror Maker 2 Active/Passive mode replication demo.","title":"Mirror Maker 2"},{"location":"mm2/#mirror-maker-2","text":"","title":"Mirror Maker 2"},{"location":"mm2/#activepassive-mirroring","text":"","title":"Active/Passive Mirroring"},{"location":"mm2/#overview","text":"This demo presents how to leverage Mirror Maker 2 between two Kafka clusters running on OpenShift. It uses two IBM Event Streams instances on both sides and utilizes mirroring feature that ships as part of IBM Event Streams Operator API's. Cluster 1 (Active): This will be our source cluster (Source). In this case it can be a Development environment where consumers and producers are connected. Cluster 2 (Passive): This will be our target cluster (Target). In this case it can be a Staging environment where no consumers or producers are connected. Upon failover, consumers will be connected to the newly promoted cluster (Cluster 2) which will become active. Mirror Maker 2 is a continuous background mirroring process and can be run in its own namespace. However, for the purposes of this demo, it will be created within the destination namespace, in our case rt-inventory-stg and connect to the source Kafka cluser, in our case it is in rt-inventory-dev namespace.","title":"Overview"},{"location":"mm2/#pre-requisites","text":"We assume, you have access to one or two separate OpenShift clusters. The OpenShift cluster(s) has/have IBM Event Streams Operator version 3.0.x installed and available for all namespaces on the cluster(s).","title":"Pre-requisites"},{"location":"mm2/#steps","text":"1- Creating Source (Origin) Kafka Cluster: Using Web Console on the first OpenShift cluster, create a new project named rt-inventory-dev Make sure that this project is selected and create a new EventStreams instance named rt-inventory-dev inside it. Note This cluster represents the Development Environment (Active) Once the EventStreams instance rt-inventory-dev is up and running, access its Admin UI to perform the below sub-tasks: Create SCRAM credentials with username rt-inv-dev-user . This will create rt-inv-dev-user secret. Take a note of the SCRAM username and password. Take a note of the bootstrap Server address. Generate TLS certificates. This will create rt-inventory-dev-cluster-ca-cert secret. Download and save the PEM certificate. You could rename it to es-src-cert.pem . Create Kafka topics items , items.inventory , and store.inventory which will be used to demonstrate replication from source to target. Note The above two created secrets will need to be copied to the target cluster so Mirror Maker 2 can reference them to connect to the source cluster 2- Creating Target (Destination) Kafka Cluster: Using Web Console on the first OpenShift cluster, create a new project named rt-inventory-stg Make sure that this project is selected and create a new EventStreams instance named rt-inventory-stg inside it. Note This cluster represents the Staging Environment (Passive) Once the EventStreams instance rt-inventory-stg is up and running, access its Admin UI to perform the below sub-tasks: Create SCRAM credentials with username rt-inv-stg-user . This will create rt-inv-stg-user secret. Take a note of the SCRAM username and password. Take a note of the bootstrap Server address. Generate TLS certificates. This will create rt-inventory-stg-cluster-ca-cert secret. Download and save the PEM certificate. You could rename it to es-tgt-cert.pem . Note Mirror Maker 2 will reference the above two created secrets to connect to the target cluster 3- Creating MirrorMaker 2 instance: Make sure that rt-inventory-stg project is selected. Create the two sercets generated from Step 1 by copying their yaml configs from rt-inventory-dev project to preserve the names. Using OpenShift Web Console (Administrator perspective), navigate to Operators -> Installed Operators. On the Installed Operators page, click on IBM Event Sreams . On the Avialable API's page, find Kafka Mirror Maker 2 then click Create instance . Select Configure via YAML view to use the yaml file provided in this demo. Change the bootstrapServers address (line 36) to be the address of your Source Kafka bootstrapServer (from Step 1). Change the bootstrapServers address (line 53) to be the address of your Target Kafka bootstrapServer (from Step 2). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 apiVersion : eventstreams.ibm.com/v1beta2 kind : KafkaMirrorMaker2 metadata : name : mm2 namespace : rt-inventory-stg spec : version : 3.0.1 replicas : 1 connectCluster : es-tgt logging : loggers : connect.root.logger.level : INFO type : inline template : pod : metadata : annotations : cloudpakId : c8b82d189e7545f0892db9ef2731b90d productVersion : 11.0.0 productID : 2a79e49111f44ec3acd89608e56138f5 cloudpakName : IBM Cloud Pak for Integration cloudpakVersion : 2022.1.1 productChargedContainers : mm2-mirrormaker2 productCloudpakRatio : '2:1' productName : IBM Event Streams for Non Production eventstreams.production.type : CloudPakForIntegrationNonProduction productMetric : VIRTUAL_PROCESSOR_CORE clusters : - alias : es-src authentication : passwordSecret : password : password secretName : rt-inv-dev-user type : scram-sha-512 username : rt-inv-dev-user bootstrapServers : rt-inventory-dev-kafka-bootstrap-rt-inventory-dev.itzroks-4b4a.us-south.containers.appdomain.cloud:443 config : ssl.endpoint.identification.algorithm : https config.storage.replication.factor : 1 offset.storage.replication.factor : 1 status.storage.replication.factor : 1 tls : trustedCertificates : - certificate : ca.crt secretName : rt-inventory-dev-cluster-ca-cert - alias : es-tgt authentication : passwordSecret : password : password secretName : rt-inv-stg-user type : scram-sha-512 username : rt-inv-stg-user bootstrapServers : rt-inventory-stg-kafka-bootstrap-rt-inventory-stg.itzroks-4b4b.us-south.containers.appdomain.cloud:443 config : ssl.endpoint.identification.algorithm : https config.storage.replication.factor : 3 offset.storage.replication.factor : 3 status.storage.replication.factor : 3 tls : trustedCertificates : - certificate : ca.crt secretName : rt-inventory-stg-cluster-ca-cert mirrors : - sourceCluster : es-src targetCluster : es-tgt sourceConnector : config : # the replication factor that will be used for # all topics created on the \"target\" Kafka cluster replication.factor : 3 # don't try to copy permissions across from the \"origin\" # cluster to the \"target\" cluster sync.topic.acls.enabled : \"false\" replication.policy.separator : \"\" replication.policy.class : \"io.strimzi.kafka.connect.mirror.IdentityReplicationPolicy\" offset-syncs.topic.replication.factor : 3 refresh.topics.interval.seconds : 10 checkpointConnector : config : checkpoints.topic.replication.factor : 3 refresh.groups.interval.seconds : 5 # migrates the consumer group offsets emit.checkpoints.enabled : true sync.group.offsets.enabled : true sync.group.offsets.interval.seconds : 5 emit.checkpoints.interval.seconds : 5 # ensures that consumer group offsets on the \"target\" cluster # are correctly mapped to consumer groups on the \"origin\" cluster replication.policy.class : \"io.strimzi.kafka.connect.mirror.IdentityReplicationPolicy\" replication.policy.separator : \"\" topicsPattern : \"items, store.inventory, items.inventory\" topicsExcludePattern : \".apicurio\" groupsPattern : \".*\" If the same namespaces and SCRAM usernames are used as indicated in the previous steps, no further changes are required. Click Create to apply the YAML changes and create KafkaMirrorMaker2 instance. In few seconds, check the status of KafkaMirrorMaker2 instance from Kafka Mirror Maker 2 Tab. The status should be Condition: Ready . KafkaMirrorMaker2 instance created mm2 will deploy different resources that can be checked from its Resources Tab. You might need to check the created Pod resource log for errors or warnings. Now, the created instance mm2 will start to mirror (replicate) the Kafka topics' events, and offsets from source to the target cluster. Only Kafka topics specified in topicsPattern will be replicated and topics specified in topicsExcludePattern will be excluded.","title":"Steps"},{"location":"mm2/#verification","text":"Access the EventStreams instance rt-inventory-stg Admin UI to verify that the replication is working. From the side menu, select Topics to see the list of Kafka topics created on our Target cluster (Staging Environment). You can see that items , items.inventory , and store.inventory Kafka topics were created and events are being replicated. Kafka Topics named mirrormaker2-cluster-xxx are used internally by our KafkaMirrorMaker2 instance to keep track of configs, offsets, and replication status. Click on items topic then visit the Messages Tab to see that the events are being replicated as they arrive to the Source cluster. The next section will demonstrate how to produce sample events to the Source cluster.","title":"Verification"},{"location":"mm2/#producing-events-source","text":"This section will be used to verify that the replication of messages (Kafka events) is actually happening (live events feed is moving from Source to Target). We will use a simple python script (accompanied with a starter bash script) that connects to the Source cluster and sends a random json payloads to the items topic created in Step 1. The producer script requires Python 3.x with confluent-kafka Kakfa library installed. To install the Kafka library, run: pip install confluent-kafka Perform the following steps to setup the producer sample application: On your local machine, create a new directory named producer . Download and save SendItems.py and sendItems.sh files inside producer directory. Move the Source cluster PEM certificate file es-src-cert.pem to the same directory. Edit the sendItems.sh script to set the environment variables of Source cluster connectivity configs. Change the KAFKA_BROKERS variable to the Source cluster bootstrap address. Change the KAFKA_PWD variable to be the password of rt-inv-dev-user SCRAM user. #!/bin/bash export KAFKA_BROKERS = rt-inventory-dev-kafka-bootstrap-rt-inventory-dev.itzroks-4b4a.us-south.containers.appdomain.cloud:443 export KAFKA_USER = rt-inv-dev-user export KAFKA_PWD = SCRAM-PASSWORD export KAFKA_CERT = es-src-cert.pem export KAFKA_SECURITY_PROTOCOL = SASL_SSL export KAFKA_SASL_MECHANISM = SCRAM-SHA-512 export SOURCE_TOPIC = items python SendItems.py $1 5. Now you can run the shell script and pass the number of events to be sent as an argument. ./sendItems.sh 10","title":"Producing Events (Source)"},{"location":"mm2/#consuming-events-target","text":"This section will simulate failover to Target (passive) cluster. MirrorMaker 2 uses checkpointConnector to automatically store consumer group offset checkpoints for consumer groups on the Source Kafka cluster. Each checkpoint maps the last committed offset for each consumer group in the Source cluster to the equivalent offset in the Target cluster. These checkpoints will be used by our Kafka consumer script to start consuming from an offset on the Target cluster that is equivalent to the last committed offset on the Source cluster. The same way we used to setup the Producer application, we need to perform the following steps to setup the Consumer application: On your local machine, create a new directory named consumer . Download and save ReceiveItems.py and receiveItems.sh files inside consumer directory. Move the Target cluster PEM certificate file es-tgt-cert.pem to the same directory. Edit the receiveItems.sh script to set the environment variables of Target cluster connectivity configs. Change the KAFKA_BROKERS variable to the Target cluster bootstrap address. Change the KAFKA_PWD variable to be the password of rt-inv-stg-user SCRAM user. #!/bin/bash export KAFKA_BROKERS = rt-inventory-stg-kafka-bootstrap-rt-inventory-stg.itzroks-4b4b.us-south.containers.appdomain.cloud:443 export KAFKA_USER = rt-inv-stg-user export KAFKA_PWD = SCRAM-PASSWORD export KAFKA_CERT = es-tgt-cert.pem export KAFKA_SECURITY_PROTOCOL = SASL_SSL export KAFKA_SASL_MECHANISM = SCRAM-SHA-512 export SOURCE_TOPIC = items python ReceiveItems.py $1 5. Now you can run the shell script and pass the number of events to be consumed as an argument. ./consumeItems.sh 10 6. You should see the mirrored messages now in items replicated topic in our Target Kafka cluster. Note To check the current and offset lag, you can use Event Streams UI on Target cluster. We can see that the consumer application has started to read from the last committed offset. This concludes our Kafka Mirror Maker 2 Active/Passive mode replication demo.","title":"Consuming Events (Target)"}]}